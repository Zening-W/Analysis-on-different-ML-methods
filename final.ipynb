{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47616</th>\n",
       "      <td>33</td>\n",
       "      <td>245211</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47617</th>\n",
       "      <td>39</td>\n",
       "      <td>215419</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47618</th>\n",
       "      <td>38</td>\n",
       "      <td>374983</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47619</th>\n",
       "      <td>44</td>\n",
       "      <td>83891</td>\n",
       "      <td>13</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47620</th>\n",
       "      <td>35</td>\n",
       "      <td>182148</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47621 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0       39   77516             13          2174             0              40   \n",
       "1       50   83311             13             0             0              13   \n",
       "2       38  215646              9             0             0              40   \n",
       "3       53  234721              7             0             0              40   \n",
       "4       28  338409             13             0             0              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "47616   33  245211             13             0             0              40   \n",
       "47617   39  215419             13             0             0              36   \n",
       "47618   38  374983             13             0             0              50   \n",
       "47619   44   83891             13          5455             0              40   \n",
       "47620   35  182148             13             0             0              60   \n",
       "\n",
       "       income  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0           0                  False                False   \n",
       "1           0                  False                False   \n",
       "2           0                  False                False   \n",
       "3           0                  False                False   \n",
       "4           0                  False                False   \n",
       "...       ...                    ...                  ...   \n",
       "47616       0                  False                False   \n",
       "47617       0                  False                False   \n",
       "47618       0                  False                False   \n",
       "47619       0                  False                False   \n",
       "47620       1                  False                False   \n",
       "\n",
       "       workclass_Never-worked  ...  native-country_Portugal  \\\n",
       "0                       False  ...                    False   \n",
       "1                       False  ...                    False   \n",
       "2                       False  ...                    False   \n",
       "3                       False  ...                    False   \n",
       "4                       False  ...                    False   \n",
       "...                       ...  ...                      ...   \n",
       "47616                   False  ...                    False   \n",
       "47617                   False  ...                    False   \n",
       "47618                   False  ...                    False   \n",
       "47619                   False  ...                    False   \n",
       "47620                   False  ...                    False   \n",
       "\n",
       "       native-country_Puerto-Rico  native-country_Scotland  \\\n",
       "0                           False                    False   \n",
       "1                           False                    False   \n",
       "2                           False                    False   \n",
       "3                           False                    False   \n",
       "4                           False                    False   \n",
       "...                           ...                      ...   \n",
       "47616                       False                    False   \n",
       "47617                       False                    False   \n",
       "47618                       False                    False   \n",
       "47619                       False                    False   \n",
       "47620                       False                    False   \n",
       "\n",
       "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
       "0                     False                  False                    False   \n",
       "1                     False                  False                    False   \n",
       "2                     False                  False                    False   \n",
       "3                     False                  False                    False   \n",
       "4                     False                  False                    False   \n",
       "...                     ...                    ...                      ...   \n",
       "47616                 False                  False                    False   \n",
       "47617                 False                  False                    False   \n",
       "47618                 False                  False                    False   \n",
       "47619                 False                  False                    False   \n",
       "47620                 False                  False                    False   \n",
       "\n",
       "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                               False                          True   \n",
       "1                               False                          True   \n",
       "2                               False                          True   \n",
       "3                               False                          True   \n",
       "4                               False                         False   \n",
       "...                               ...                           ...   \n",
       "47616                           False                          True   \n",
       "47617                           False                          True   \n",
       "47618                           False                          True   \n",
       "47619                           False                          True   \n",
       "47620                           False                          True   \n",
       "\n",
       "       native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                       False                      False  \n",
       "1                       False                      False  \n",
       "2                       False                      False  \n",
       "3                       False                      False  \n",
       "4                       False                      False  \n",
       "...                       ...                        ...  \n",
       "47616                   False                      False  \n",
       "47617                   False                      False  \n",
       "47618                   False                      False  \n",
       "47619                   False                      False  \n",
       "47620                   False                      False  \n",
       "\n",
       "[47621 rows x 101 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "adult_X = adult.data.features \n",
    "adult_y = adult.data.targets \n",
    "  \n",
    "# variable information \n",
    "adult_y\n",
    "# Combine features and target into one DataFrame\n",
    "adult_df = pd.concat([adult_X, adult_y], axis=1)\n",
    "\n",
    "# Drop rows with missing values\n",
    "adult_df.dropna(inplace=True)\n",
    "adult_df.reset_index(drop=True, inplace=True)\n",
    "adult_df\n",
    "\n",
    "# Identify categorical columns (replace 'categorical_column1', 'categorical_column2', etc. with your actual column names)\n",
    "categorical_columns = ['workclass', 'education','marital-status','occupation','relationship','race','sex','native-country']\n",
    "\n",
    "# Create a DataFrame with the original categorical columns\n",
    "categorical_df = adult_df[categorical_columns]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_categorical_df = pd.get_dummies(categorical_df, drop_first=True)\n",
    "\n",
    "# Replace the original categorical columns with the encoded ones\n",
    "adult_df = pd.concat([adult_df.drop(categorical_columns, axis=1), encoded_categorical_df], axis=1)\n",
    "\n",
    "\n",
    "# Convert the income to binary results\n",
    "income_mapping = {'<=50K': 0, '>50K': 1,'<=50K.': 0,  '>50K.': 1}\n",
    "adult_df['income'] = adult_df['income'].map(income_mapping)\n",
    "\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (38096, 100) (38096, 100) (38096, 100)\n",
      "X_test shape: (9525, 100) (9525, 100) (9525, 100)\n",
      "y_train shape: (38096,) (38096,) (38096,)\n",
      "y_test shape: (9525,) (9525,) (9525,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = adult_df.drop('income', axis=1)  \n",
    "y = adult_df['income']\n",
    "\n",
    "# Split the data into training and testing sets. However, we also need to consider that using first 80% for training,\n",
    "# rest 20% for testing is different from first 20% for training, rest for testing. So we would 3 times each for \n",
    "# every partition and compute average scores to remove potentials of having accidental results. We would set shuffle=true,\n",
    "# and use different random_state each time.\n",
    "\n",
    "# 80% training, 20% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.11798860267273278\n",
      "Root Mean Squared Error: 0.34349469089453594\n",
      "R-squared: 0.3660427695984775\n",
      "Mean Squared Error2: 0.11694971718553746\n",
      "Root Mean Squared Error: 0.34197911805479797\n",
      "R-squared: 0.36946172366283914\n",
      "Mean Squared Error3: 0.11517773359958065\n",
      "Root Mean Squared Error: 0.3393784518786964\n",
      "R-squared: 0.3612221984621845\n",
      "Average Mean Squared Error of 3 trials: 0.11670535115261697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7921259842519685\n",
      "Confusion Matrix:\n",
      " [[6718  452]\n",
      " [1528  827]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      7170\n",
      "           1       0.65      0.35      0.46      2355\n",
      "\n",
      "    accuracy                           0.79      9525\n",
      "   macro avg       0.73      0.64      0.66      9525\n",
      "weighted avg       0.77      0.79      0.77      9525\n",
      "\n",
      "Accuracy: 0.796010498687664\n",
      "Confusion Matrix:\n",
      " [[6948  234]\n",
      " [1709  634]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88      7182\n",
      "           1       0.73      0.27      0.39      2343\n",
      "\n",
      "    accuracy                           0.80      9525\n",
      "   macro avg       0.77      0.62      0.64      9525\n",
      "weighted avg       0.78      0.80      0.76      9525\n",
      "\n",
      "Accuracy: 0.8006299212598426\n",
      "Confusion Matrix:\n",
      " [[7029  248]\n",
      " [1651  597]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88      7277\n",
      "           1       0.71      0.27      0.39      2248\n",
      "\n",
      "    accuracy                           0.80      9525\n",
      "   macro avg       0.76      0.62      0.63      9525\n",
      "weighted avg       0.79      0.80      0.76      9525\n",
      "\n",
      "average mean of accuracy three trials: 0.7962554680664917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7911811023622047\n",
      "Confusion Matrix:\n",
      " [[7151   19]\n",
      " [1970  385]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      7170\n",
      "           1       0.95      0.16      0.28      2355\n",
      "\n",
      "    accuracy                           0.79      9525\n",
      "   macro avg       0.87      0.58      0.58      9525\n",
      "weighted avg       0.83      0.79      0.73      9525\n",
      "\n",
      "Accuracy: 0.7952755905511811\n",
      "Confusion Matrix:\n",
      " [[7168   14]\n",
      " [1936  407]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      7182\n",
      "           1       0.97      0.17      0.29      2343\n",
      "\n",
      "    accuracy                           0.80      9525\n",
      "   macro avg       0.88      0.59      0.59      9525\n",
      "weighted avg       0.83      0.80      0.74      9525\n",
      "\n",
      "Accuracy: 0.8028346456692913\n",
      "Confusion Matrix:\n",
      " [[7265   12]\n",
      " [1866  382]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89      7277\n",
      "           1       0.97      0.17      0.29      2248\n",
      "\n",
      "    accuracy                           0.80      9525\n",
      "   macro avg       0.88      0.58      0.59      9525\n",
      "weighted avg       0.84      0.80      0.74      9525\n",
      "\n",
      "average mean of accuracy three trials: 0.7964304461942256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix1 = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep1 = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix1)\n",
    "print(\"Classification Report:\\n\", classification_rep1)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix2 = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep2 = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix2)\n",
    "print(\"Classification Report:\\n\", classification_rep2)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix3 = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep3 = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix3)\n",
    "print(\"Classification Report:\\n\", classification_rep3)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8489238845144357\n",
      "Confusion Matrix:\n",
      " [[6678  492]\n",
      " [ 947 1408]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90      7170\n",
      "           1       0.74      0.60      0.66      2355\n",
      "\n",
      "    accuracy                           0.85      9525\n",
      "   macro avg       0.81      0.76      0.78      9525\n",
      "weighted avg       0.84      0.85      0.84      9525\n",
      "\n",
      "Accuracy: 0.8490288713910761\n",
      "Confusion Matrix:\n",
      " [[6615  567]\n",
      " [ 871 1472]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      7182\n",
      "           1       0.72      0.63      0.67      2343\n",
      "\n",
      "    accuracy                           0.85      9525\n",
      "   macro avg       0.80      0.77      0.79      9525\n",
      "weighted avg       0.84      0.85      0.85      9525\n",
      "\n",
      "Accuracy: 0.8028346456692913\n",
      "Confusion Matrix:\n",
      " [[6708  569]\n",
      " [ 845 1403]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      7277\n",
      "           1       0.71      0.62      0.66      2248\n",
      "\n",
      "    accuracy                           0.85      9525\n",
      "   macro avg       0.80      0.77      0.78      9525\n",
      "weighted avg       0.85      0.85      0.85      9525\n",
      "\n",
      "average mean of accuracy three trials: 0.8344356955380577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'X' and 'y' are your feature and target variables\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}  # Adjust the range as needed\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_neighbor = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best n_neighbors:\", best_neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7857217847769029\n",
      "Confusion Matrix:\n",
      " [[6835  335]\n",
      " [1706  649]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87      7170\n",
      "           1       0.66      0.28      0.39      2355\n",
      "\n",
      "    accuracy                           0.79      9525\n",
      "   macro avg       0.73      0.61      0.63      9525\n",
      "weighted avg       0.77      0.79      0.75      9525\n",
      "\n",
      "Accuracy: 0.7903412073490813\n",
      "Confusion Matrix:\n",
      " [[6817  365]\n",
      " [1632  711]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87      7182\n",
      "           1       0.66      0.30      0.42      2343\n",
      "\n",
      "    accuracy                           0.79      9525\n",
      "   macro avg       0.73      0.63      0.64      9525\n",
      "weighted avg       0.77      0.79      0.76      9525\n",
      "\n",
      "Accuracy: 0.7935958005249344\n",
      "Confusion Matrix:\n",
      " [[6922  355]\n",
      " [1611  637]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88      7277\n",
      "           1       0.64      0.28      0.39      2248\n",
      "\n",
      "    accuracy                           0.79      9525\n",
      "   macro avg       0.73      0.62      0.63      9525\n",
      "weighted avg       0.77      0.79      0.76      9525\n",
      "\n",
      "average mean of accuracy three trials: 0.7898862642169728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (23810, 100) (23810, 100) (23810, 100)\n",
      "X_test shape: (23811, 100) (23811, 100) (23811, 100)\n",
      "y_train shape: (23810,) (23810,) (23810,)\n",
      "y_test shape: (23811,) (23811,) (23811,)\n"
     ]
    }
   ],
   "source": [
    "# 50% training, 50% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.11682593910853742\n",
      "Root Mean Squared Error: 0.34179809699373315\n",
      "R-squared: 0.36756582793672365\n",
      "Mean Squared Error2: 0.11665032670634408\n",
      "Root Mean Squared Error: 0.34154110544170824\n",
      "R-squared: 0.36270464405226\n",
      "Mean Squared Error3: 0.11655367147558676\n",
      "Root Mean Squared Error: 0.3413995774390864\n",
      "R-squared: 0.36533807077801916\n",
      "Average Mean Squared Error of 3 trials: 0.11667664576348942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy1: 0.7942547562051153\n",
      "Confusion Matrix:\n",
      " [[17399   590]\n",
      " [ 4309  1513]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     17989\n",
      "           1       0.72      0.26      0.38      5822\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.76      0.61      0.63     23811\n",
      "weighted avg       0.78      0.79      0.76     23811\n",
      "\n",
      "Accuracy2: 0.7966486077863173\n",
      "Confusion Matrix:\n",
      " [[17421   646]\n",
      " [ 4196  1548]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88     18067\n",
      "           1       0.71      0.27      0.39      5744\n",
      "\n",
      "    accuracy                           0.80     23811\n",
      "   macro avg       0.76      0.62      0.63     23811\n",
      "weighted avg       0.78      0.80      0.76     23811\n",
      "\n",
      "Accuracy3: 0.7978245348788375\n",
      "Confusion Matrix:\n",
      " [[17488   551]\n",
      " [ 4263  1509]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     18039\n",
      "           1       0.73      0.26      0.39      5772\n",
      "\n",
      "    accuracy                           0.80     23811\n",
      "   macro avg       0.77      0.62      0.63     23811\n",
      "weighted avg       0.79      0.80      0.76     23811\n",
      "\n",
      "average mean of accuracy three trials: 0.7962426329567567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy1:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy2:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy3:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78971903741968\n",
      "Confusion Matrix:\n",
      " [[17980     9]\n",
      " [ 4998   824]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     17989\n",
      "           1       0.99      0.14      0.25      5822\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.89      0.57      0.56     23811\n",
      "weighted avg       0.83      0.79      0.72     23811\n",
      "\n",
      "Accuracy: 0.7942547562051153\n",
      "Confusion Matrix:\n",
      " [[18052    15]\n",
      " [ 4884   860]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88     18067\n",
      "           1       0.98      0.15      0.26      5744\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.88      0.57      0.57     23811\n",
      "weighted avg       0.83      0.79      0.73     23811\n",
      "\n",
      "Accuracy: 0.7978245348788375\n",
      "Confusion Matrix:\n",
      " [[18031     8]\n",
      " [ 4922   850]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88     18039\n",
      "           1       0.99      0.15      0.26      5772\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.89      0.57      0.57     23811\n",
      "weighted avg       0.84      0.79      0.73     23811\n",
      "\n",
      "average mean of accuracy three trials: 0.7950107093360211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851287220192348\n",
      "Confusion Matrix:\n",
      " [[16730  1259]\n",
      " [ 2282  3540]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     17989\n",
      "           1       0.74      0.61      0.67      5822\n",
      "\n",
      "    accuracy                           0.85     23811\n",
      "   macro avg       0.81      0.77      0.79     23811\n",
      "weighted avg       0.85      0.85      0.85     23811\n",
      "\n",
      "Accuracy: 0.8528831212464827\n",
      "Confusion Matrix:\n",
      " [[16743  1324]\n",
      " [ 2179  3565]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91     18067\n",
      "           1       0.73      0.62      0.67      5744\n",
      "\n",
      "    accuracy                           0.85     23811\n",
      "   macro avg       0.81      0.77      0.79     23811\n",
      "weighted avg       0.85      0.85      0.85     23811\n",
      "\n",
      "Accuracy: 0.7978245348788375\n",
      "Confusion Matrix:\n",
      " [[16673  1366]\n",
      " [ 2143  3629]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90     18039\n",
      "           1       0.73      0.63      0.67      5772\n",
      "\n",
      "    accuracy                           0.85     23811\n",
      "   macro avg       0.81      0.78      0.79     23811\n",
      "weighted avg       0.85      0.85      0.85     23811\n",
      "\n",
      "average mean of accuracy three trials: 0.8339142973135666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7877031624039309\n",
      "Confusion Matrix:\n",
      " [[17300   689]\n",
      " [ 4366  1456]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     17989\n",
      "           1       0.68      0.25      0.37      5822\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.74      0.61      0.62     23811\n",
      "weighted avg       0.77      0.79      0.75     23811\n",
      "\n",
      "Accuracy: 0.7904749905505859\n",
      "Confusion Matrix:\n",
      " [[17309   758]\n",
      " [ 4231  1513]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     18067\n",
      "           1       0.67      0.26      0.38      5744\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.73      0.61      0.63     23811\n",
      "weighted avg       0.77      0.79      0.75     23811\n",
      "\n",
      "Accuracy: 0.7909369619083617\n",
      "Confusion Matrix:\n",
      " [[17325   714]\n",
      " [ 4264  1508]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     18039\n",
      "           1       0.68      0.26      0.38      5772\n",
      "\n",
      "    accuracy                           0.79     23811\n",
      "   macro avg       0.74      0.61      0.63     23811\n",
      "weighted avg       0.77      0.79      0.75     23811\n",
      "\n",
      "average mean of accuracy three trials: 0.7897050382876262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14286, 100) (14286, 100) (14286, 100)\n",
      "X_test shape: (33335, 100) (33335, 100) (33335, 100)\n",
      "y_train shape: (14286,) (14286,) (14286,)\n",
      "y_test shape: (33335,) (33335,) (33335,)\n"
     ]
    }
   ],
   "source": [
    "# 30% training, 70% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.11736153766801749\n",
      "Root Mean Squared Error: 0.3425807024162591\n",
      "R-squared: 0.36229463054578104\n",
      "Mean Squared Error2: 0.11726632989097546\n",
      "Root Mean Squared Error: 0.3424417175096741\n",
      "R-squared: 0.3603957368634936\n",
      "Mean Squared Error3: 0.11689162514299879\n",
      "Root Mean Squared Error: 0.3418941724320536\n",
      "R-squared: 0.36254718561458965\n",
      "Average Mean Squared Error of 3 trials: 0.11717316423399725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7967901604919754\n",
      "Confusion Matrix:\n",
      " [[24410   819]\n",
      " [ 5955  2151]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     25229\n",
      "           1       0.72      0.27      0.39      8106\n",
      "\n",
      "    accuracy                           0.80     33335\n",
      "   macro avg       0.76      0.62      0.63     33335\n",
      "weighted avg       0.78      0.80      0.76     33335\n",
      "\n",
      "Accuracy: 0.7952002399880006\n",
      "Confusion Matrix:\n",
      " [[24351   923]\n",
      " [ 5904  2157]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.88     25274\n",
      "           1       0.70      0.27      0.39      8061\n",
      "\n",
      "    accuracy                           0.80     33335\n",
      "   macro avg       0.75      0.62      0.63     33335\n",
      "weighted avg       0.78      0.80      0.76     33335\n",
      "\n",
      "Accuracy: 0.7971201439928004\n",
      "Confusion Matrix:\n",
      " [[24439   833]\n",
      " [ 5930  2133]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     25272\n",
      "           1       0.72      0.26      0.39      8063\n",
      "\n",
      "    accuracy                           0.80     33335\n",
      "   macro avg       0.76      0.62      0.63     33335\n",
      "weighted avg       0.78      0.80      0.76     33335\n",
      "\n",
      "average mean of accuracy three trials: 0.7963701814909254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.783920803959802\n",
      "Confusion Matrix:\n",
      " [[25218    11]\n",
      " [ 7192   914]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     25229\n",
      "           1       0.99      0.11      0.20      8106\n",
      "\n",
      "    accuracy                           0.78     33335\n",
      "   macro avg       0.88      0.56      0.54     33335\n",
      "weighted avg       0.83      0.78      0.71     33335\n",
      "\n",
      "Accuracy: 0.7863506824658767\n",
      "Confusion Matrix:\n",
      " [[25261    13]\n",
      " [ 7109   952]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     25274\n",
      "           1       0.99      0.12      0.21      8061\n",
      "\n",
      "    accuracy                           0.79     33335\n",
      "   macro avg       0.88      0.56      0.54     33335\n",
      "weighted avg       0.83      0.79      0.72     33335\n",
      "\n",
      "Accuracy: 0.7971201439928004\n",
      "Confusion Matrix:\n",
      " [[25261    11]\n",
      " [ 7118   945]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     25272\n",
      "           1       0.99      0.12      0.21      8063\n",
      "\n",
      "    accuracy                           0.79     33335\n",
      "   macro avg       0.88      0.56      0.54     33335\n",
      "weighted avg       0.83      0.79      0.72     33335\n",
      "\n",
      "average mean of accuracy three trials: 0.7898705064746764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8507874606269686\n",
      "Confusion Matrix:\n",
      " [[23363  1866]\n",
      " [ 3108  4998]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     25229\n",
      "           1       0.73      0.62      0.67      8106\n",
      "\n",
      "    accuracy                           0.85     33335\n",
      "   macro avg       0.81      0.77      0.79     33335\n",
      "weighted avg       0.85      0.85      0.85     33335\n",
      "\n",
      "Accuracy: 0.8510874456277187\n",
      "Confusion Matrix:\n",
      " [[23439  1835]\n",
      " [ 3129  4932]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     25274\n",
      "           1       0.73      0.61      0.67      8061\n",
      "\n",
      "    accuracy                           0.85     33335\n",
      "   macro avg       0.81      0.77      0.78     33335\n",
      "weighted avg       0.85      0.85      0.85     33335\n",
      "\n",
      "Accuracy: 0.7971201439928004\n",
      "Confusion Matrix:\n",
      " [[23410  1862]\n",
      " [ 3048  5015]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91     25272\n",
      "           1       0.73      0.62      0.67      8063\n",
      "\n",
      "    accuracy                           0.85     33335\n",
      "   macro avg       0.81      0.77      0.79     33335\n",
      "weighted avg       0.85      0.85      0.85     33335\n",
      "\n",
      "average mean of accuracy three trials: 0.8335383230838458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7884805759712015\n",
      "Confusion Matrix:\n",
      " [[24437   792]\n",
      " [ 6259  1847]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.87     25229\n",
      "           1       0.70      0.23      0.34      8106\n",
      "\n",
      "    accuracy                           0.79     33335\n",
      "   macro avg       0.75      0.60      0.61     33335\n",
      "weighted avg       0.77      0.79      0.75     33335\n",
      "\n",
      "Accuracy: 0.7870706464676767\n",
      "Confusion Matrix:\n",
      " [[24275   999]\n",
      " [ 6099  1962]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     25274\n",
      "           1       0.66      0.24      0.36      8061\n",
      "\n",
      "    accuracy                           0.79     33335\n",
      "   macro avg       0.73      0.60      0.61     33335\n",
      "weighted avg       0.77      0.79      0.75     33335\n",
      "\n",
      "Accuracy: 0.7895905204739763\n",
      "Confusion Matrix:\n",
      " [[24381   891]\n",
      " [ 6123  1940]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     25272\n",
      "           1       0.69      0.24      0.36      8063\n",
      "\n",
      "    accuracy                           0.79     33335\n",
      "   macro avg       0.74      0.60      0.62     33335\n",
      "weighted avg       0.77      0.79      0.75     33335\n",
      "\n",
      "average mean of accuracy three trials: 0.7883805809709514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   1       145   233    1        2      150      0      2.3   \n",
       "1     67    1   4       160   286    0        2      108      1      1.5   \n",
       "2     67    1   4       120   229    0        2      129      1      2.6   \n",
       "3     37    1   3       130   250    0        0      187      0      3.5   \n",
       "4     41    0   2       130   204    0        2      172      0      1.4   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "292   57    0   4       140   241    0        0      123      1      0.2   \n",
       "293   45    1   1       110   264    0        0      132      0      1.2   \n",
       "294   68    1   4       144   193    1        0      141      0      3.4   \n",
       "295   57    1   4       130   131    0        0      115      1      1.2   \n",
       "296   57    0   2       130   236    0        2      174      0      0.0   \n",
       "\n",
       "     slope   ca  thal  num  \n",
       "0        3  0.0   6.0    0  \n",
       "1        2  3.0   3.0    2  \n",
       "2        2  2.0   7.0    1  \n",
       "3        3  0.0   3.0    0  \n",
       "4        1  0.0   3.0    0  \n",
       "..     ...  ...   ...  ...  \n",
       "292      2  0.0   7.0    1  \n",
       "293      2  0.0   7.0    1  \n",
       "294      2  2.0   7.0    2  \n",
       "295      2  1.0   7.0    3  \n",
       "296      2  1.0   3.0    1  \n",
       "\n",
       "[297 rows x 14 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "heart_X = heart_disease.data.features \n",
    "heart_y = heart_disease.data.targets \n",
    "  \n",
    "  \n",
    "# variable information \n",
    "#print(heart_disease.variables) \n",
    "heart_y\n",
    "# Combine features and target into one DataFrame\n",
    "heart_df = pd.concat([heart_X, heart_y], axis=1)\n",
    "\n",
    "# Drop rows with missing values\n",
    "heart_df.dropna(inplace=True)\n",
    "heart_df.reset_index(drop=True, inplace=True)\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (237, 13) (237, 13) (237, 13)\n",
      "X_test shape: (60, 13) (60, 13) (60, 13)\n",
      "y_train shape: (237,) (237,) (237,)\n",
      "y_test shape: (60,) (60,) (60,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = heart_df.drop('num', axis=1)  \n",
    "y = heart_df['num']\n",
    "\n",
    "# Split the data into training and testing sets. However, we also need to consider that using first 80% for training,\n",
    "# rest 20% for testing is different from first 20% for training, rest for testing. So we would 3 times each for \n",
    "# every partition and compute average scores to remove potentials of having accidental results. We would set shuffle=true,\n",
    "# and use different random_state each time.\n",
    "\n",
    "# 80% training, 20% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.8203721866929784\n",
      "Root Mean Squared Error: 0.9057439962224306\n",
      "R-squared: 0.4815063426799997\n",
      "Mean Squared Error2: 0.6966698453679048\n",
      "Root Mean Squared Error: 0.8346675058775829\n",
      "R-squared: 0.5672111400648046\n",
      "Mean Squared Error3: 0.4557153968984596\n",
      "Root Mean Squared Error: 0.675066957344573\n",
      "R-squared: 0.6314141925781949\n",
      "Average Mean Squared Error of 3 trials: 0.6575858096531143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Confusion Matrix:\n",
      " [[35  0  0  1  0]\n",
      " [ 4  3  1  1  0]\n",
      " [ 0  2  1  2  0]\n",
      " [ 3  2  1  1  0]\n",
      " [ 2  0  1  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88        36\n",
      "           1       0.43      0.33      0.38         9\n",
      "           2       0.25      0.20      0.22         5\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.33      0.33      0.33        60\n",
      "weighted avg       0.59      0.67      0.62        60\n",
      "\n",
      "Accuracy: 0.6333333333333333\n",
      "Confusion Matrix:\n",
      " [[34  1  0  0  0]\n",
      " [ 3  3  0  2  0]\n",
      " [ 1  3  0  3  0]\n",
      " [ 1  3  2  1  0]\n",
      " [ 0  2  0  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        35\n",
      "           1       0.25      0.38      0.30         8\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.14      0.14      0.14         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63        60\n",
      "   macro avg       0.25      0.30      0.27        60\n",
      "weighted avg       0.56      0.63      0.59        60\n",
      "\n",
      "Accuracy: 0.7\n",
      "Confusion Matrix:\n",
      " [[34  1  0  0  0]\n",
      " [ 6  5  0  1  0]\n",
      " [ 0  2  2  1  0]\n",
      " [ 2  1  3  1  0]\n",
      " [ 0  0  1  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88        35\n",
      "           1       0.56      0.42      0.48        12\n",
      "           2       0.33      0.40      0.36         5\n",
      "           3       0.33      0.14      0.20         7\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.41      0.39      0.38        60\n",
      "weighted avg       0.65      0.70      0.66        60\n",
      "\n",
      "average mean of accuracy three trials: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Confusion Matrix:\n",
      " [[36  0  0  0  0]\n",
      " [ 9  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 7  0  0  0  0]\n",
      " [ 3  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75        36\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60        60\n",
      "   macro avg       0.12      0.20      0.15        60\n",
      "weighted avg       0.36      0.60      0.45        60\n",
      "\n",
      "Accuracy: 0.5833333333333334\n",
      "Confusion Matrix:\n",
      " [[35  0  0  0  0]\n",
      " [ 8  0  0  0  0]\n",
      " [ 7  0  0  0  0]\n",
      " [ 7  0  0  0  0]\n",
      " [ 3  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74        35\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.58        60\n",
      "   macro avg       0.12      0.20      0.15        60\n",
      "weighted avg       0.34      0.58      0.43        60\n",
      "\n",
      "Accuracy: 0.7\n",
      "Confusion Matrix:\n",
      " [[35  0  0  0  0]\n",
      " [12  0  0  0  0]\n",
      " [ 5  0  0  0  0]\n",
      " [ 7  0  0  0  0]\n",
      " [ 1  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74        35\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.58        60\n",
      "   macro avg       0.12      0.20      0.15        60\n",
      "weighted avg       0.34      0.58      0.43        60\n",
      "\n",
      "average mean of accuracy three trials: 0.6222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6333333333333333\n",
      "Confusion Matrix:\n",
      " [[35  1  0  0  0]\n",
      " [ 6  1  2  0  0]\n",
      " [ 1  1  2  1  0]\n",
      " [ 2  4  0  0  1]\n",
      " [ 1  2  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86        36\n",
      "           1       0.11      0.11      0.11         9\n",
      "           2       0.50      0.40      0.44         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.63        60\n",
      "   macro avg       0.28      0.30      0.28        60\n",
      "weighted avg       0.53      0.63      0.57        60\n",
      "\n",
      "Accuracy: 0.65\n",
      "Confusion Matrix:\n",
      " [[33  1  1  0  0]\n",
      " [ 4  1  1  2  0]\n",
      " [ 2  0  2  3  0]\n",
      " [ 1  2  1  3  0]\n",
      " [ 0  1  1  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88        35\n",
      "           1       0.20      0.12      0.15         8\n",
      "           2       0.33      0.29      0.31         7\n",
      "           3       0.33      0.43      0.38         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.65        60\n",
      "   macro avg       0.34      0.36      0.34        60\n",
      "weighted avg       0.59      0.65      0.61        60\n",
      "\n",
      "Accuracy: 0.7\n",
      "Confusion Matrix:\n",
      " [[35  0  0  0  0]\n",
      " [ 7  2  1  2  0]\n",
      " [ 2  0  2  1  0]\n",
      " [ 2  3  1  1  0]\n",
      " [ 0  0  0  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        35\n",
      "           1       0.40      0.17      0.24        12\n",
      "           2       0.50      0.40      0.44         5\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.37      0.34      0.34        60\n",
      "weighted avg       0.59      0.67      0.61        60\n",
      "\n",
      "average mean of accuracy three trials: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'X' and 'y' are your feature and target variables\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}  # Adjust the range as needed\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_neighbor = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best n_neighbors:\", best_neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5666666666666667\n",
      "Confusion Matrix:\n",
      " [[33  1  0  2  0]\n",
      " [ 6  1  1  1  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 7  0  0  0  0]\n",
      " [ 3  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.92      0.75        36\n",
      "           1       0.25      0.11      0.15         9\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.57        60\n",
      "   macro avg       0.18      0.21      0.18        60\n",
      "weighted avg       0.42      0.57      0.47        60\n",
      "\n",
      "Accuracy: 0.48333333333333334\n",
      "Confusion Matrix:\n",
      " [[28  6  1  0  0]\n",
      " [ 5  1  1  1  0]\n",
      " [ 5  2  0  0  0]\n",
      " [ 6  0  1  0  0]\n",
      " [ 3  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.80      0.68        35\n",
      "           1       0.11      0.12      0.12         8\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.48        60\n",
      "   macro avg       0.14      0.18      0.16        60\n",
      "weighted avg       0.36      0.48      0.41        60\n",
      "\n",
      "Accuracy: 0.5666666666666667\n",
      "Confusion Matrix:\n",
      " [[34  1  0  0  0]\n",
      " [11  0  0  0  1]\n",
      " [ 3  1  0  1  0]\n",
      " [ 6  1  0  0  0]\n",
      " [ 1  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.97      0.76        35\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.57        60\n",
      "   macro avg       0.12      0.19      0.15        60\n",
      "weighted avg       0.36      0.57      0.44        60\n",
      "\n",
      "average mean of accuracy three trials: 0.5388888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (148, 13) (148, 13) (148, 13)\n",
      "X_test shape: (149, 13) (149, 13) (149, 13)\n",
      "y_train shape: (148,) (148,) (148,)\n",
      "y_test shape: (149,) (149,) (149,)\n"
     ]
    }
   ],
   "source": [
    "# 50% training, 50% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.7690552519852273\n",
      "Root Mean Squared Error: 0.876957953373608\n",
      "R-squared: 0.5046479154774275\n",
      "Mean Squared Error2: 0.8752143579212527\n",
      "Root Mean Squared Error: 0.9355289188054278\n",
      "R-squared: 0.41268788658536637\n",
      "Mean Squared Error3: 0.7457771841259108\n",
      "Root Mean Squared Error: 0.8635839184039446\n",
      "R-squared: 0.526482890099544\n",
      "Average Mean Squared Error of 3 trials: 0.7966822646774636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5369127516778524\n",
      "Confusion Matrix:\n",
      " [[66  1  1  2  0]\n",
      " [19  4  3  7  0]\n",
      " [ 2  5  5  7  0]\n",
      " [ 5  6  4  5  0]\n",
      " [ 2  2  2  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80        70\n",
      "           1       0.22      0.12      0.16        33\n",
      "           2       0.33      0.26      0.29        19\n",
      "           3       0.23      0.25      0.24        20\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.54       149\n",
      "   macro avg       0.30      0.32      0.30       149\n",
      "weighted avg       0.45      0.54      0.48       149\n",
      "\n",
      "Accuracy: 0.5906040268456376\n",
      "Confusion Matrix:\n",
      " [[70  6  3  3  6]\n",
      " [10  5  4  3  1]\n",
      " [ 1  4  6  3  1]\n",
      " [ 2  3  4  7  1]\n",
      " [ 1  0  1  4  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81        88\n",
      "           1       0.28      0.22      0.24        23\n",
      "           2       0.33      0.40      0.36        15\n",
      "           3       0.35      0.41      0.38        17\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       149\n",
      "   macro avg       0.36      0.36      0.36       149\n",
      "weighted avg       0.61      0.59      0.60       149\n",
      "\n",
      "Accuracy: 0.6040268456375839\n",
      "Confusion Matrix:\n",
      " [[77  2  0  1  1]\n",
      " [16  5  4  2  0]\n",
      " [ 3  6  5  1  0]\n",
      " [ 7  2  7  2  1]\n",
      " [ 1  0  3  2  1]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83        81\n",
      "           1       0.33      0.19      0.24        27\n",
      "           2       0.26      0.33      0.29        15\n",
      "           3       0.25      0.11      0.15        19\n",
      "           4       0.33      0.14      0.20         7\n",
      "\n",
      "    accuracy                           0.60       149\n",
      "   macro avg       0.38      0.34      0.34       149\n",
      "weighted avg       0.54      0.60      0.55       149\n",
      "\n",
      "average mean of accuracy three trials: 0.5771812080536913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4697986577181208\n",
      "Confusion Matrix:\n",
      " [[70  0  0  0  0]\n",
      " [33  0  0  0  0]\n",
      " [19  0  0  0  0]\n",
      " [20  0  0  0  0]\n",
      " [ 7  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        70\n",
      "           1       0.00      0.00      0.00        33\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.47       149\n",
      "   macro avg       0.09      0.20      0.13       149\n",
      "weighted avg       0.22      0.47      0.30       149\n",
      "\n",
      "Accuracy: 0.5906040268456376\n",
      "Confusion Matrix:\n",
      " [[88  0  0  0  0]\n",
      " [23  0  0  0  0]\n",
      " [15  0  0  0  0]\n",
      " [17  0  0  0  0]\n",
      " [ 6  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74        88\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       149\n",
      "   macro avg       0.12      0.20      0.15       149\n",
      "weighted avg       0.35      0.59      0.44       149\n",
      "\n",
      "Accuracy: 0.6040268456375839\n",
      "Confusion Matrix:\n",
      " [[81  0  0  0  0]\n",
      " [27  0  0  0  0]\n",
      " [15  0  0  0  0]\n",
      " [19  0  0  0  0]\n",
      " [ 7  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        81\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.00      0.00      0.00        19\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.54       149\n",
      "   macro avg       0.11      0.20      0.14       149\n",
      "weighted avg       0.30      0.54      0.38       149\n",
      "\n",
      "average mean of accuracy three trials: 0.5794183445190156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5100671140939598\n",
      "Confusion Matrix:\n",
      " [[68  1  0  1  0]\n",
      " [21  0  5  7  0]\n",
      " [ 9  0  7  2  1]\n",
      " [ 9  2  7  1  1]\n",
      " [ 3  1  3  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.97      0.76        70\n",
      "           1       0.00      0.00      0.00        33\n",
      "           2       0.32      0.37      0.34        19\n",
      "           3       0.09      0.05      0.06        20\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.51       149\n",
      "   macro avg       0.21      0.28      0.23       149\n",
      "weighted avg       0.34      0.51      0.41       149\n",
      "\n",
      "Accuracy: 0.5838926174496645\n",
      "Confusion Matrix:\n",
      " [[73 12  2  1  0]\n",
      " [ 9  6  3  5  0]\n",
      " [ 2  6  4  3  0]\n",
      " [ 2  7  3  4  1]\n",
      " [ 1  3  0  2  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83        88\n",
      "           1       0.18      0.26      0.21        23\n",
      "           2       0.33      0.27      0.30        15\n",
      "           3       0.27      0.24      0.25        17\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.58       149\n",
      "   macro avg       0.32      0.32      0.32       149\n",
      "weighted avg       0.59      0.58      0.58       149\n",
      "\n",
      "Accuracy: 0.6040268456375839\n",
      "Confusion Matrix:\n",
      " [[76  4  1  0  0]\n",
      " [20  2  1  4  0]\n",
      " [ 6  3  5  1  0]\n",
      " [ 5  9  3  2  0]\n",
      " [ 1  0  1  5  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80        81\n",
      "           1       0.11      0.07      0.09        27\n",
      "           2       0.45      0.33      0.38        15\n",
      "           3       0.17      0.11      0.13        19\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.57       149\n",
      "   macro avg       0.29      0.29      0.28       149\n",
      "weighted avg       0.47      0.57      0.51       149\n",
      "\n",
      "average mean of accuracy three trials: 0.5615212527964205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44966442953020136\n",
      "Confusion Matrix:\n",
      " [[67  2  0  1  0]\n",
      " [29  0  1  3  0]\n",
      " [14  1  0  4  0]\n",
      " [17  2  1  0  0]\n",
      " [ 7  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66        70\n",
      "           1       0.00      0.00      0.00        33\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.45       149\n",
      "   macro avg       0.10      0.19      0.13       149\n",
      "weighted avg       0.23      0.45      0.31       149\n",
      "\n",
      "Accuracy: 0.5167785234899329\n",
      "Confusion Matrix:\n",
      " [[73 11  3  1  0]\n",
      " [13  3  3  4  0]\n",
      " [11  1  0  3  0]\n",
      " [11  4  0  1  1]\n",
      " [ 2  2  2  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74        88\n",
      "           1       0.14      0.13      0.14        23\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.11      0.06      0.08        17\n",
      "           4       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.52       149\n",
      "   macro avg       0.18      0.20      0.19       149\n",
      "weighted avg       0.43      0.52      0.47       149\n",
      "\n",
      "Accuracy: 0.4966442953020134\n",
      "Confusion Matrix:\n",
      " [[69 10  2  0  0]\n",
      " [21  4  1  1  0]\n",
      " [12  3  0  0  0]\n",
      " [14  3  1  1  0]\n",
      " [ 5  0  1  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.85      0.68        81\n",
      "           1       0.20      0.15      0.17        27\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.33      0.05      0.09        19\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.50       149\n",
      "   macro avg       0.22      0.21      0.19       149\n",
      "weighted avg       0.39      0.50      0.41       149\n",
      "\n",
      "average mean of accuracy three trials: 0.4876957494407159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (89, 13) (89, 13) (89, 13)\n",
      "X_test shape: (208, 13) (208, 13) (208, 13)\n",
      "y_train shape: (89,) (89,) (89,)\n",
      "y_test shape: (208,) (208,) (208,)\n"
     ]
    }
   ],
   "source": [
    "# 30% training, 70% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.7961364821251968\n",
      "Root Mean Squared Error: 0.8922648049347216\n",
      "R-squared: 0.4586992588215909\n",
      "Mean Squared Error2: 0.8803855149184874\n",
      "Root Mean Squared Error: 0.9382886096071333\n",
      "R-squared: 0.3791625414837013\n",
      "Mean Squared Error3: 0.7947988736957152\n",
      "Root Mean Squared Error: 0.8915149318411415\n",
      "R-squared: 0.496917697304042\n",
      "Average Mean Squared Error of 3 trials: 0.8237736235797998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5769230769230769\n",
      "Confusion Matrix:\n",
      " [[102   0   3   0   0]\n",
      " [ 28   3   8   5   0]\n",
      " [ 10   3   6   6   0]\n",
      " [  9   3   5   9   0]\n",
      " [  2   1   3   2   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.97      0.80       105\n",
      "           1       0.30      0.07      0.11        44\n",
      "           2       0.24      0.24      0.24        25\n",
      "           3       0.41      0.35      0.38        26\n",
      "           4       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.58       208\n",
      "   macro avg       0.32      0.33      0.30       208\n",
      "weighted avg       0.48      0.58      0.50       208\n",
      "\n",
      "Accuracy: 0.5817307692307693\n",
      "Confusion Matrix:\n",
      " [[94 10  5  3  7]\n",
      " [15 10  8  3  1]\n",
      " [ 1  4 10  5  1]\n",
      " [ 2  6  7  7  2]\n",
      " [ 2  1  3  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       119\n",
      "           1       0.32      0.27      0.29        37\n",
      "           2       0.30      0.48      0.37        21\n",
      "           3       0.37      0.29      0.33        24\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.58       208\n",
      "   macro avg       0.36      0.37      0.36       208\n",
      "weighted avg       0.60      0.58      0.59       208\n",
      "\n",
      "Accuracy: 0.5336538461538461\n",
      "Confusion Matrix:\n",
      " [[96 10  1  1  6]\n",
      " [19  8  4  2  1]\n",
      " [ 3 13  3  5  1]\n",
      " [ 4 14  4  3  0]\n",
      " [ 1  3  1  4  1]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       114\n",
      "           1       0.17      0.24      0.20        34\n",
      "           2       0.23      0.12      0.16        25\n",
      "           3       0.20      0.12      0.15        25\n",
      "           4       0.11      0.10      0.11        10\n",
      "\n",
      "    accuracy                           0.53       208\n",
      "   macro avg       0.30      0.28      0.28       208\n",
      "weighted avg       0.51      0.53      0.52       208\n",
      "\n",
      "average mean of accuracy three trials: 0.5641025641025642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5048076923076923\n",
      "Confusion Matrix:\n",
      " [[105   0   0   0   0]\n",
      " [ 44   0   0   0   0]\n",
      " [ 25   0   0   0   0]\n",
      " [ 26   0   0   0   0]\n",
      " [  8   0   0   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       105\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.00      0.00      0.00        25\n",
      "           3       0.00      0.00      0.00        26\n",
      "           4       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.50       208\n",
      "   macro avg       0.10      0.20      0.13       208\n",
      "weighted avg       0.25      0.50      0.34       208\n",
      "\n",
      "Accuracy: 0.5721153846153846\n",
      "Confusion Matrix:\n",
      " [[119   0   0   0   0]\n",
      " [ 37   0   0   0   0]\n",
      " [ 21   0   0   0   0]\n",
      " [ 24   0   0   0   0]\n",
      " [  7   0   0   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73       119\n",
      "           1       0.00      0.00      0.00        37\n",
      "           2       0.00      0.00      0.00        21\n",
      "           3       0.00      0.00      0.00        24\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.57       208\n",
      "   macro avg       0.11      0.20      0.15       208\n",
      "weighted avg       0.33      0.57      0.42       208\n",
      "\n",
      "Accuracy: 0.5336538461538461\n",
      "Confusion Matrix:\n",
      " [[114   0   0   0   0]\n",
      " [ 34   0   0   0   0]\n",
      " [ 25   0   0   0   0]\n",
      " [ 25   0   0   0   0]\n",
      " [ 10   0   0   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       114\n",
      "           1       0.00      0.00      0.00        34\n",
      "           2       0.00      0.00      0.00        25\n",
      "           3       0.00      0.00      0.00        25\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.55       208\n",
      "   macro avg       0.11      0.20      0.14       208\n",
      "weighted avg       0.30      0.55      0.39       208\n",
      "\n",
      "average mean of accuracy three trials: 0.5512820512820512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5576923076923077\n",
      "Confusion Matrix:\n",
      " [[102   1   2   0   0]\n",
      " [ 31   0   9   4   0]\n",
      " [ 11   2   9   3   0]\n",
      " [ 10   2   8   4   2]\n",
      " [  2   1   3   1   1]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.97      0.78       105\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.29      0.36      0.32        25\n",
      "           3       0.33      0.15      0.21        26\n",
      "           4       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.56       208\n",
      "   macro avg       0.32      0.32      0.30       208\n",
      "weighted avg       0.42      0.56      0.47       208\n",
      "\n",
      "Accuracy: 0.6153846153846154\n",
      "Confusion Matrix:\n",
      " [[108   7   2   2   0]\n",
      " [ 22   3   4   7   1]\n",
      " [  2   4  10   4   1]\n",
      " [  3   6   7   7   1]\n",
      " [  1   1   2   3   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       119\n",
      "           1       0.14      0.08      0.10        37\n",
      "           2       0.40      0.48      0.43        21\n",
      "           3       0.30      0.29      0.30        24\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.62       208\n",
      "   macro avg       0.33      0.35      0.34       208\n",
      "weighted avg       0.56      0.62      0.58       208\n",
      "\n",
      "Accuracy: 0.5336538461538461\n",
      "Confusion Matrix:\n",
      " [[100  11   2   1   0]\n",
      " [ 20  11   2   1   0]\n",
      " [  9  10   3   3   0]\n",
      " [  7  13   4   1   0]\n",
      " [  1   4   2   3   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       114\n",
      "           1       0.22      0.32      0.27        34\n",
      "           2       0.23      0.12      0.16        25\n",
      "           3       0.11      0.04      0.06        25\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.55       208\n",
      "   macro avg       0.26      0.27      0.26       208\n",
      "weighted avg       0.48      0.55      0.51       208\n",
      "\n",
      "average mean of accuracy three trials: 0.548076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4567307692307692\n",
      "Confusion Matrix:\n",
      " [[92 10  3  0  0]\n",
      " [40  0  2  2  0]\n",
      " [21  0  3  1  0]\n",
      " [25  0  1  0  0]\n",
      " [ 7  1  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.88      0.63       105\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.33      0.12      0.18        25\n",
      "           3       0.00      0.00      0.00        26\n",
      "           4       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.46       208\n",
      "   macro avg       0.17      0.20      0.16       208\n",
      "weighted avg       0.29      0.46      0.34       208\n",
      "\n",
      "Accuracy: 0.5192307692307693\n",
      "Confusion Matrix:\n",
      " [[102   5  10   2   0]\n",
      " [ 29   2   0   5   1]\n",
      " [ 17   0   1   3   0]\n",
      " [ 18   1   0   3   2]\n",
      " [  4   1   2   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.86      0.71       119\n",
      "           1       0.22      0.05      0.09        37\n",
      "           2       0.08      0.05      0.06        21\n",
      "           3       0.23      0.12      0.16        24\n",
      "           4       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.52       208\n",
      "   macro avg       0.23      0.22      0.20       208\n",
      "weighted avg       0.42      0.52      0.44       208\n",
      "\n",
      "Accuracy: 0.5192307692307693\n",
      "Confusion Matrix:\n",
      " [[98 15  1  0  0]\n",
      " [27  5  0  2  0]\n",
      " [15  6  4  0  0]\n",
      " [19  4  1  1  0]\n",
      " [ 7  1  1  1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.86      0.70       114\n",
      "           1       0.16      0.15      0.15        34\n",
      "           2       0.57      0.16      0.25        25\n",
      "           3       0.25      0.04      0.07        25\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.52       208\n",
      "   macro avg       0.31      0.24      0.23       208\n",
      "weighted avg       0.45      0.52      0.45       208\n",
      "\n",
      "average mean of accuracy three trials: 0.49839743589743596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>0D280_0D315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
       "0      14.23       1.71  2.43               15.6        127           2.80   \n",
       "1      13.20       1.78  2.14               11.2        100           2.65   \n",
       "2      13.16       2.36  2.67               18.6        101           2.80   \n",
       "3      14.37       1.95  2.50               16.8        113           3.85   \n",
       "4      13.24       2.59  2.87               21.0        118           2.80   \n",
       "..       ...        ...   ...                ...        ...            ...   \n",
       "173    13.71       5.65  2.45               20.5         95           1.68   \n",
       "174    13.40       3.91  2.48               23.0        102           1.80   \n",
       "175    13.27       4.28  2.26               20.0        120           1.59   \n",
       "176    13.17       2.59  2.37               20.0        120           1.65   \n",
       "177    14.13       4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     0D280_0D315_of_diluted_wines  Proline  class  \n",
       "0                            3.92     1065      1  \n",
       "1                            3.40     1050      1  \n",
       "2                            3.17     1185      1  \n",
       "3                            3.45     1480      1  \n",
       "4                            2.93      735      1  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74      740      3  \n",
       "174                          1.56      750      3  \n",
       "175                          1.56      835      3  \n",
       "176                          1.62      840      3  \n",
       "177                          1.60      560      3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  \n",
    "# fetch dataset \n",
    "wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "wine_X = wine.data.features \n",
    "wine_y = wine.data.targets \n",
    "  \n",
    "# Combine features and target into one DataFrame\n",
    "wine_df = pd.concat([wine_X, wine_y], axis=1)\n",
    "\n",
    "# Drop rows with missing values\n",
    "wine_df.dropna(inplace=True)\n",
    "wine_df.reset_index(drop=True, inplace=True)\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (142, 13) (142, 13) (142, 13)\n",
      "X_test shape: (36, 13) (36, 13) (36, 13)\n",
      "y_train shape: (142,) (142,) (142,)\n",
      "y_test shape: (36,) (36,) (36,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = wine_df.drop('class', axis=1)  \n",
    "y = wine_df['class']\n",
    "\n",
    "# Split the data into training and testing sets. However, we also need to consider that using first 80% for training,\n",
    "# rest 20% for testing is different from first 20% for training, rest for testing. So we would 3 times each for \n",
    "# every partition and compute average scores to remove potentials of having accidental results. We would set shuffle=true,\n",
    "# and use different random_state each time.\n",
    "\n",
    "# 80% training, 20% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.06853348464256041\n",
      "Root Mean Squared Error: 0.2617890078719128\n",
      "R-squared: 0.8825140263270392\n",
      "Mean Squared Error2: 0.06999745491730155\n",
      "Root Mean Squared Error: 0.2645703213085352\n",
      "R-squared: 0.8591355565639397\n",
      "Mean Squared Error3: 0.097378733133621\n",
      "Root Mean Squared Error: 0.3120556571088257\n",
      "R-squared: 0.8398441140340447\n",
      "Average Mean Squared Error of 3 trials: 0.07863655756449432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n",
      "Confusion Matrix:\n",
      " [[13  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.93      1.00      0.97        14\n",
      "           3       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 1 15  2]\n",
      " [ 0  0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      1.00      0.95        10\n",
      "           2       1.00      0.83      0.91        18\n",
      "           3       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.90      0.94      0.92        36\n",
      "weighted avg       0.93      0.92      0.92        36\n",
      "\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[11  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  2  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.82      1.00      0.90        14\n",
      "           3       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.94      0.91      0.92        36\n",
      "weighted avg       0.93      0.92      0.92        36\n",
      "\n",
      "average mean of accuracy three trials: 0.9351851851851851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8055555555555556\n",
      "Confusion Matrix:\n",
      " [[14  0  0]\n",
      " [ 0 11  3]\n",
      " [ 0  4  4]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.73      0.79      0.76        14\n",
      "           3       0.57      0.50      0.53         8\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.77      0.76      0.76        36\n",
      "weighted avg       0.80      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.6944444444444444\n",
      "Confusion Matrix:\n",
      " [[ 7  0  3]\n",
      " [ 1 13  4]\n",
      " [ 0  3  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.70      0.78        10\n",
      "           2       0.81      0.72      0.76        18\n",
      "           3       0.42      0.62      0.50         8\n",
      "\n",
      "    accuracy                           0.69        36\n",
      "   macro avg       0.70      0.68      0.68        36\n",
      "weighted avg       0.74      0.69      0.71        36\n",
      "\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[ 9  0  3]\n",
      " [ 2 10  2]\n",
      " [ 0  7  3]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.75      0.78        12\n",
      "           2       0.59      0.71      0.65        14\n",
      "           3       0.38      0.30      0.33        10\n",
      "\n",
      "    accuracy                           0.61        36\n",
      "   macro avg       0.59      0.59      0.59        36\n",
      "weighted avg       0.61      0.61      0.60        36\n",
      "\n",
      "average mean of accuracy three trials: 0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        14\n",
      "           3       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 15  3]\n",
      " [ 0  0  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      0.83      0.91        18\n",
      "           3       0.73      1.00      0.84         8\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.91      0.94      0.92        36\n",
      "weighted avg       0.94      0.92      0.92        36\n",
      "\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[11  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.93      1.00      0.97        14\n",
      "           3       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.97      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "average mean of accuracy three trials: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'X' and 'y' are your feature and target variables\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}  # Adjust the range as needed\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_neighbor = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best n_neighbors:\", best_neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777777777777778\n",
      "Confusion Matrix:\n",
      " [[12  0  2]\n",
      " [ 3 11  0]\n",
      " [ 1  2  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.86      0.80        14\n",
      "           2       0.85      0.79      0.81        14\n",
      "           3       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.77      0.76      0.76        36\n",
      "weighted avg       0.78      0.78      0.78        36\n",
      "\n",
      "Accuracy: 0.6944444444444444\n",
      "Confusion Matrix:\n",
      " [[ 9  0  1]\n",
      " [ 4 11  3]\n",
      " [ 0  3  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.90      0.78        10\n",
      "           2       0.79      0.61      0.69        18\n",
      "           3       0.56      0.62      0.59         8\n",
      "\n",
      "    accuracy                           0.69        36\n",
      "   macro avg       0.68      0.71      0.69        36\n",
      "weighted avg       0.71      0.69      0.69        36\n",
      "\n",
      "Accuracy: 0.6944444444444444\n",
      "Confusion Matrix:\n",
      " [[ 9  1  2]\n",
      " [ 1 10  3]\n",
      " [ 1  3  6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.75      0.78        12\n",
      "           2       0.71      0.71      0.71        14\n",
      "           3       0.55      0.60      0.57        10\n",
      "\n",
      "    accuracy                           0.69        36\n",
      "   macro avg       0.69      0.69      0.69        36\n",
      "weighted avg       0.70      0.69      0.70        36\n",
      "\n",
      "average mean of accuracy three trials: 0.7222222222222223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (89, 13) (89, 13) (89, 13)\n",
      "X_test shape: (89, 13) (89, 13) (89, 13)\n",
      "y_train shape: (89,) (89,) (89,)\n",
      "y_test shape: (89,) (89,) (89,)\n"
     ]
    }
   ],
   "source": [
    "# 50% training, 50% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.08290159844081665\n",
      "Root Mean Squared Error: 0.28792637677159183\n",
      "R-squared: 0.8624500290637392\n",
      "Mean Squared Error2: 0.09824282950970675\n",
      "Root Mean Squared Error: 0.3134371220990053\n",
      "R-squared: 0.8375404065665162\n",
      "Mean Squared Error3: 0.0905871982559387\n",
      "Root Mean Squared Error: 0.3009770726416527\n",
      "R-squared: 0.8314047938474411\n",
      "Average Mean Squared Error of 3 trials: 0.09057720873548736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9325842696629213\n",
      "Confusion Matrix:\n",
      " [[30  3  0]\n",
      " [ 2 32  0]\n",
      " [ 0  1 21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.91      0.92        33\n",
      "           2       0.89      0.94      0.91        34\n",
      "           3       1.00      0.95      0.98        22\n",
      "\n",
      "    accuracy                           0.93        89\n",
      "   macro avg       0.94      0.93      0.94        89\n",
      "weighted avg       0.93      0.93      0.93        89\n",
      "\n",
      "Accuracy: 0.8876404494382022\n",
      "Confusion Matrix:\n",
      " [[27  2  0]\n",
      " [ 6 27  2]\n",
      " [ 0  0 25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.93      0.87        29\n",
      "           2       0.93      0.77      0.84        35\n",
      "           3       0.93      1.00      0.96        25\n",
      "\n",
      "    accuracy                           0.89        89\n",
      "   macro avg       0.89      0.90      0.89        89\n",
      "weighted avg       0.89      0.89      0.89        89\n",
      "\n",
      "Accuracy: 0.9213483146067416\n",
      "Confusion Matrix:\n",
      " [[25  1  0]\n",
      " [ 3 36  2]\n",
      " [ 0  1 21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.96      0.93        26\n",
      "           2       0.95      0.88      0.91        41\n",
      "           3       0.91      0.95      0.93        22\n",
      "\n",
      "    accuracy                           0.92        89\n",
      "   macro avg       0.92      0.93      0.92        89\n",
      "weighted avg       0.92      0.92      0.92        89\n",
      "\n",
      "average mean of accuracy three trials: 0.9138576779026217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.651685393258427\n",
      "Confusion Matrix:\n",
      " [[28  0  5]\n",
      " [ 1 23 10]\n",
      " [ 0 15  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.85      0.90        33\n",
      "           2       0.61      0.68      0.64        34\n",
      "           3       0.32      0.32      0.32        22\n",
      "\n",
      "    accuracy                           0.65        89\n",
      "   macro avg       0.63      0.61      0.62        89\n",
      "weighted avg       0.67      0.65      0.66        89\n",
      "\n",
      "Accuracy: 0.6292134831460674\n",
      "Confusion Matrix:\n",
      " [[22  1  6]\n",
      " [ 2 29  4]\n",
      " [ 1 19  5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.76      0.81        29\n",
      "           2       0.59      0.83      0.69        35\n",
      "           3       0.33      0.20      0.25        25\n",
      "\n",
      "    accuracy                           0.63        89\n",
      "   macro avg       0.60      0.60      0.59        89\n",
      "weighted avg       0.61      0.63      0.61        89\n",
      "\n",
      "Accuracy: 0.9213483146067416\n",
      "Confusion Matrix:\n",
      " [[22  0  4]\n",
      " [ 2 26 13]\n",
      " [ 0 11 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.85      0.88        26\n",
      "           2       0.70      0.63      0.67        41\n",
      "           3       0.39      0.50      0.44        22\n",
      "\n",
      "    accuracy                           0.66        89\n",
      "   macro avg       0.67      0.66      0.66        89\n",
      "weighted avg       0.69      0.66      0.67        89\n",
      "\n",
      "average mean of accuracy three trials: 0.7378277153558052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9775280898876404\n",
      "Confusion Matrix:\n",
      " [[31  2  0]\n",
      " [ 0 34  0]\n",
      " [ 0  0 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.94      0.97        33\n",
      "           2       0.94      1.00      0.97        34\n",
      "           3       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.98        89\n",
      "   macro avg       0.98      0.98      0.98        89\n",
      "weighted avg       0.98      0.98      0.98        89\n",
      "\n",
      "Accuracy: 0.9325842696629213\n",
      "Confusion Matrix:\n",
      " [[27  2  0]\n",
      " [ 1 31  3]\n",
      " [ 0  0 25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.93      0.95        29\n",
      "           2       0.94      0.89      0.91        35\n",
      "           3       0.89      1.00      0.94        25\n",
      "\n",
      "    accuracy                           0.93        89\n",
      "   macro avg       0.93      0.94      0.93        89\n",
      "weighted avg       0.93      0.93      0.93        89\n",
      "\n",
      "Accuracy: 0.9213483146067416\n",
      "Confusion Matrix:\n",
      " [[26  0  0]\n",
      " [ 0 39  2]\n",
      " [ 0  0 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        26\n",
      "           2       1.00      0.95      0.97        41\n",
      "           3       0.92      1.00      0.96        22\n",
      "\n",
      "    accuracy                           0.98        89\n",
      "   macro avg       0.97      0.98      0.98        89\n",
      "weighted avg       0.98      0.98      0.98        89\n",
      "\n",
      "average mean of accuracy three trials: 0.9588014981273408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6853932584269663\n",
      "Confusion Matrix:\n",
      " [[27  2  4]\n",
      " [ 4 24  6]\n",
      " [ 1 11 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.82      0.83        33\n",
      "           2       0.65      0.71      0.68        34\n",
      "           3       0.50      0.45      0.48        22\n",
      "\n",
      "    accuracy                           0.69        89\n",
      "   macro avg       0.66      0.66      0.66        89\n",
      "weighted avg       0.68      0.69      0.68        89\n",
      "\n",
      "Accuracy: 0.7303370786516854\n",
      "Confusion Matrix:\n",
      " [[21  3  5]\n",
      " [ 4 28  3]\n",
      " [ 0  9 16]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.72      0.78        29\n",
      "           2       0.70      0.80      0.75        35\n",
      "           3       0.67      0.64      0.65        25\n",
      "\n",
      "    accuracy                           0.73        89\n",
      "   macro avg       0.74      0.72      0.73        89\n",
      "weighted avg       0.74      0.73      0.73        89\n",
      "\n",
      "Accuracy: 0.7078651685393258\n",
      "Confusion Matrix:\n",
      " [[23  1  2]\n",
      " [ 3 29  9]\n",
      " [ 1 10 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.88      0.87        26\n",
      "           2       0.72      0.71      0.72        41\n",
      "           3       0.50      0.50      0.50        22\n",
      "\n",
      "    accuracy                           0.71        89\n",
      "   macro avg       0.69      0.70      0.69        89\n",
      "weighted avg       0.71      0.71      0.71        89\n",
      "\n",
      "average mean of accuracy three trials: 0.7078651685393259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 13) (53, 13) (53, 13)\n",
      "X_test shape: (125, 13) (125, 13) (125, 13)\n",
      "y_train shape: (53,) (53,) (53,)\n",
      "y_test shape: (125,) (125,) (125,)\n"
     ]
    }
   ],
   "source": [
    "# 30% training, 70% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.10259876164714439\n",
      "Root Mean Squared Error: 0.3203104145155827\n",
      "R-squared: 0.825862953428565\n",
      "Mean Squared Error2: 0.08592441979926915\n",
      "Root Mean Squared Error: 0.2931286744746565\n",
      "R-squared: 0.8581393639725718\n",
      "Mean Squared Error3: 0.09787171461038278\n",
      "Root Mean Squared Error: 0.3128445534293074\n",
      "R-squared: 0.8169883268564826\n",
      "Average Mean Squared Error of 3 trials: 0.09546496535226545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.936\n",
      "Confusion Matrix:\n",
      " [[41  3  0]\n",
      " [ 2 47  1]\n",
      " [ 0  2 29]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.93      0.94        44\n",
      "           2       0.90      0.94      0.92        50\n",
      "           3       0.97      0.94      0.95        31\n",
      "\n",
      "    accuracy                           0.94       125\n",
      "   macro avg       0.94      0.94      0.94       125\n",
      "weighted avg       0.94      0.94      0.94       125\n",
      "\n",
      "Accuracy: 0.896\n",
      "Confusion Matrix:\n",
      " [[34  7  0]\n",
      " [ 3 44  2]\n",
      " [ 0  1 34]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.83      0.87        41\n",
      "           2       0.85      0.90      0.87        49\n",
      "           3       0.94      0.97      0.96        35\n",
      "\n",
      "    accuracy                           0.90       125\n",
      "   macro avg       0.90      0.90      0.90       125\n",
      "weighted avg       0.90      0.90      0.90       125\n",
      "\n",
      "Accuracy: 0.896\n",
      "Confusion Matrix:\n",
      " [[35  4  1]\n",
      " [ 3 49  5]\n",
      " [ 0  0 28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.88      0.90        40\n",
      "           2       0.92      0.86      0.89        57\n",
      "           3       0.82      1.00      0.90        28\n",
      "\n",
      "    accuracy                           0.90       125\n",
      "   macro avg       0.89      0.91      0.90       125\n",
      "weighted avg       0.90      0.90      0.90       125\n",
      "\n",
      "average mean of accuracy three trials: 0.9093333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688\n",
      "Confusion Matrix:\n",
      " [[37  0  7]\n",
      " [ 2 38 10]\n",
      " [ 0 20 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.84      0.89        44\n",
      "           2       0.66      0.76      0.70        50\n",
      "           3       0.39      0.35      0.37        31\n",
      "\n",
      "    accuracy                           0.69       125\n",
      "   macro avg       0.67      0.65      0.66       125\n",
      "weighted avg       0.69      0.69      0.69       125\n",
      "\n",
      "Accuracy: 0.64\n",
      "Confusion Matrix:\n",
      " [[33  8  0]\n",
      " [ 2 47  0]\n",
      " [ 1 34  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.80      0.86        41\n",
      "           2       0.53      0.96      0.68        49\n",
      "           3       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.64       125\n",
      "   macro avg       0.48      0.59      0.51       125\n",
      "weighted avg       0.51      0.64      0.55       125\n",
      "\n",
      "Accuracy: 0.896\n",
      "Confusion Matrix:\n",
      " [[34  0  6]\n",
      " [ 4 31 22]\n",
      " [ 0  6 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.85      0.87        40\n",
      "           2       0.84      0.54      0.66        57\n",
      "           3       0.44      0.79      0.56        28\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.72      0.73      0.70       125\n",
      "weighted avg       0.77      0.70      0.71       125\n",
      "\n",
      "average mean of accuracy three trials: 0.7439999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.944\n",
      "Confusion Matrix:\n",
      " [[42  2  0]\n",
      " [ 2 47  1]\n",
      " [ 0  2 29]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.95      0.95        44\n",
      "           2       0.92      0.94      0.93        50\n",
      "           3       0.97      0.94      0.95        31\n",
      "\n",
      "    accuracy                           0.94       125\n",
      "   macro avg       0.95      0.94      0.95       125\n",
      "weighted avg       0.94      0.94      0.94       125\n",
      "\n",
      "Accuracy: 0.936\n",
      "Confusion Matrix:\n",
      " [[36  5  0]\n",
      " [ 1 46  2]\n",
      " [ 0  0 35]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.88      0.92        41\n",
      "           2       0.90      0.94      0.92        49\n",
      "           3       0.95      1.00      0.97        35\n",
      "\n",
      "    accuracy                           0.94       125\n",
      "   macro avg       0.94      0.94      0.94       125\n",
      "weighted avg       0.94      0.94      0.94       125\n",
      "\n",
      "Accuracy: 0.896\n",
      "Confusion Matrix:\n",
      " [[38  2  0]\n",
      " [ 2 49  6]\n",
      " [ 0  0 28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.95      0.95        40\n",
      "           2       0.96      0.86      0.91        57\n",
      "           3       0.82      1.00      0.90        28\n",
      "\n",
      "    accuracy                           0.92       125\n",
      "   macro avg       0.91      0.94      0.92       125\n",
      "weighted avg       0.93      0.92      0.92       125\n",
      "\n",
      "average mean of accuracy three trials: 0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.672\n",
      "Confusion Matrix:\n",
      " [[36  5  3]\n",
      " [ 5 35 10]\n",
      " [ 2 16 13]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.82      0.83        44\n",
      "           2       0.62      0.70      0.66        50\n",
      "           3       0.50      0.42      0.46        31\n",
      "\n",
      "    accuracy                           0.67       125\n",
      "   macro avg       0.65      0.65      0.65       125\n",
      "weighted avg       0.67      0.67      0.67       125\n",
      "\n",
      "Accuracy: 0.696\n",
      "Confusion Matrix:\n",
      " [[30  5  6]\n",
      " [ 1 32 16]\n",
      " [ 0 10 25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.73      0.83        41\n",
      "           2       0.68      0.65      0.67        49\n",
      "           3       0.53      0.71      0.61        35\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.73      0.70      0.70       125\n",
      "weighted avg       0.73      0.70      0.71       125\n",
      "\n",
      "Accuracy: 0.704\n",
      "Confusion Matrix:\n",
      " [[35  0  5]\n",
      " [ 5 35 17]\n",
      " [ 0 10 18]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.88      0.88        40\n",
      "           2       0.78      0.61      0.69        57\n",
      "           3       0.45      0.64      0.53        28\n",
      "\n",
      "    accuracy                           0.70       125\n",
      "   macro avg       0.70      0.71      0.70       125\n",
      "weighted avg       0.74      0.70      0.71       125\n",
      "\n",
      "average mean of accuracy three trials: 0.6906666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "# Create DataFrame from Iris dataset\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_target = pd.DataFrame(data=iris.target, columns=['target'])\n",
    "iris_df = pd.concat([iris_data, iris_target], axis=1)\n",
    "\n",
    "iris_df.dropna(inplace=True)\n",
    "iris_df.reset_index(drop=True, inplace=True)\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (120, 4) (120, 4) (120, 4)\n",
      "X_test shape: (30, 4) (30, 4) (30, 4)\n",
      "y_train shape: (120,) (120,) (120,)\n",
      "y_test shape: (30,) (30,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = iris_df.drop('target', axis=1)  \n",
    "y = iris_df['target']\n",
    "\n",
    "# Split the data into training and testing sets. However, we also need to consider that using first 80% for training,\n",
    "# rest 20% for testing is different from first 20% for training, rest for testing. So we would 3 times each for \n",
    "# every partition and compute average scores to remove potentials of having accidental results. We would set shuffle=true,\n",
    "# and use different random_state each time.\n",
    "\n",
    "# 80% training, 20% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.03711379440797689\n",
      "Root Mean Squared Error: 0.1926494080135646\n",
      "R-squared: 0.9468960016420045\n",
      "Mean Squared Error2: 0.04921147699310945\n",
      "Root Mean Squared Error: 0.22183659976006992\n",
      "R-squared: 0.9115961491141746\n",
      "Mean Squared Error3: 0.04371010442232157\n",
      "Root Mean Squared Error: 0.20906961621029865\n",
      "R-squared: 0.9339948087582393\n",
      "Average Mean Squared Error of 3 trials: 0.0433451252744693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "average mean of accuracy three trials: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "average mean of accuracy three trials: 0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "average mean of accuracy three trials: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'X' and 'y' are your feature and target variables\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}  # Adjust the range as needed\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "best_neighbor = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Print the results of the grid search\n",
    "print(\"Best n_neighbors:\", best_neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.89      0.94         9\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0  7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "average mean of accuracy three trials: 0.9777777777777779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (75, 4) (75, 4) (75, 4)\n",
      "X_test shape: (75, 4) (75, 4) (75, 4)\n",
      "y_train shape: (75,) (75,) (75,)\n",
      "y_test shape: (75,) (75,) (75,)\n"
     ]
    }
   ],
   "source": [
    "# 50% training, 50% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.5, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.04034157363532383\n",
      "Root Mean Squared Error: 0.20085211882209217\n",
      "R-squared: 0.9412729421069626\n",
      "Mean Squared Error2: 0.05910121064094707\n",
      "Root Mean Squared Error: 0.24310740556582613\n",
      "R-squared: 0.907551637971266\n",
      "Mean Squared Error3: 0.05727711046775983\n",
      "Root Mean Squared Error: 0.23932636809963048\n",
      "R-squared: 0.9104049648550754\n",
      "Average Mean Squared Error of 3 trials: 0.05223996491467691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[29  0  0]\n",
      " [ 0 23  0]\n",
      " [ 0  0 23]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        23\n",
      "           2       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        75\n",
      "   macro avg       1.00      1.00      1.00        75\n",
      "weighted avg       1.00      1.00      1.00        75\n",
      "\n",
      "Accuracy: 0.96\n",
      "Confusion Matrix:\n",
      " [[23  0  0]\n",
      " [ 0 24  3]\n",
      " [ 0  0 25]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      0.89      0.94        27\n",
      "           2       0.89      1.00      0.94        25\n",
      "\n",
      "    accuracy                           0.96        75\n",
      "   macro avg       0.96      0.96      0.96        75\n",
      "weighted avg       0.96      0.96      0.96        75\n",
      "\n",
      "Accuracy: 0.9333333333333333\n",
      "Confusion Matrix:\n",
      " [[25  0  0]\n",
      " [ 0 25  2]\n",
      " [ 0  3 20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.89      0.93      0.91        27\n",
      "           2       0.91      0.87      0.89        23\n",
      "\n",
      "    accuracy                           0.93        75\n",
      "   macro avg       0.93      0.93      0.93        75\n",
      "weighted avg       0.93      0.93      0.93        75\n",
      "\n",
      "average mean of accuracy three trials: 0.9644444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[29  0  0]\n",
      " [ 0 23  0]\n",
      " [ 0  0 23]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        23\n",
      "           2       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        75\n",
      "   macro avg       1.00      1.00      1.00        75\n",
      "weighted avg       1.00      1.00      1.00        75\n",
      "\n",
      "Accuracy: 0.9466666666666667\n",
      "Confusion Matrix:\n",
      " [[23  0  0]\n",
      " [ 0 24  3]\n",
      " [ 0  1 24]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       0.96      0.89      0.92        27\n",
      "           2       0.89      0.96      0.92        25\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.95      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "Accuracy: 0.9333333333333333\n",
      "Confusion Matrix:\n",
      " [[25  0  0]\n",
      " [ 0 26  1]\n",
      " [ 0  2 21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.93      0.96      0.95        27\n",
      "           2       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.96        75\n",
      "   macro avg       0.96      0.96      0.96        75\n",
      "weighted avg       0.96      0.96      0.96        75\n",
      "\n",
      "average mean of accuracy three trials: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n",
      "Confusion Matrix:\n",
      " [[29  0  0]\n",
      " [ 0 23  0]\n",
      " [ 0  2 21]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       0.92      1.00      0.96        23\n",
      "           2       1.00      0.91      0.95        23\n",
      "\n",
      "    accuracy                           0.97        75\n",
      "   macro avg       0.97      0.97      0.97        75\n",
      "weighted avg       0.98      0.97      0.97        75\n",
      "\n",
      "Accuracy: 0.9466666666666667\n",
      "Confusion Matrix:\n",
      " [[23  0  0]\n",
      " [ 0 24  3]\n",
      " [ 0  1 24]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       0.96      0.89      0.92        27\n",
      "           2       0.89      0.96      0.92        25\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.95      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "Accuracy: 0.9333333333333333\n",
      "Confusion Matrix:\n",
      " [[25  0  0]\n",
      " [ 0 25  2]\n",
      " [ 0  4 19]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.86      0.93      0.89        27\n",
      "           2       0.90      0.83      0.86        23\n",
      "\n",
      "    accuracy                           0.92        75\n",
      "   macro avg       0.92      0.92      0.92        75\n",
      "weighted avg       0.92      0.92      0.92        75\n",
      "\n",
      "average mean of accuracy three trials: 0.9422222222222224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n",
      "Confusion Matrix:\n",
      " [[29  0  0]\n",
      " [ 0 23  0]\n",
      " [ 0  4 19]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       0.85      1.00      0.92        23\n",
      "           2       1.00      0.83      0.90        23\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.94      0.94        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "Accuracy: 0.9333333333333333\n",
      "Confusion Matrix:\n",
      " [[23  0  0]\n",
      " [ 0 23  4]\n",
      " [ 0  1 24]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       0.96      0.85      0.90        27\n",
      "           2       0.86      0.96      0.91        25\n",
      "\n",
      "    accuracy                           0.93        75\n",
      "   macro avg       0.94      0.94      0.94        75\n",
      "weighted avg       0.94      0.93      0.93        75\n",
      "\n",
      "Accuracy: 0.96\n",
      "Confusion Matrix:\n",
      " [[25  0  0]\n",
      " [ 0 27  0]\n",
      " [ 0  3 20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.90      1.00      0.95        27\n",
      "           2       1.00      0.87      0.93        23\n",
      "\n",
      "    accuracy                           0.96        75\n",
      "   macro avg       0.97      0.96      0.96        75\n",
      "weighted avg       0.96      0.96      0.96        75\n",
      "\n",
      "average mean of accuracy three trials: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45, 4) (45, 4) (45, 4)\n",
      "X_test shape: (105, 4) (105, 4) (105, 4)\n",
      "y_train shape: (45,) (45,) (45,)\n",
      "y_test shape: (105,) (105,) (105,)\n"
     ]
    }
   ],
   "source": [
    "# 30% training, 70% testing partition\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=10)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.7, shuffle= True, random_state=35)\n",
    "\n",
    "# Print the shape of the resulting sets\n",
    "print(\"X_train shape:\", X_train1.shape, X_train2.shape, X_train3.shape)\n",
    "print(\"X_test shape:\", X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "print(\"y_train shape:\", y_train1.shape, y_train2.shape, y_train3.shape)\n",
    "print(\"y_test shape:\", y_test1.shape, y_test2.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error1: 0.04985988920765808\n",
      "Root Mean Squared Error: 0.22329328070423007\n",
      "R-squared: 0.9266668518523973\n",
      "Mean Squared Error2: 0.051153871247161205\n",
      "Root Mean Squared Error: 0.22617221590452088\n",
      "R-squared: 0.9220603329878452\n",
      "Mean Squared Error3: 0.05294585420762915\n",
      "Root Mean Squared Error: 0.23009966146787167\n",
      "R-squared: 0.918061757069187\n",
      "Average Mean Squared Error of 3 trials: 0.051319871554149486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse1 = mean_squared_error(y_test1, y_pred)\n",
    "rmse = np.sqrt(mse1)\n",
    "r2 = r2_score(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse2 = mean_squared_error(y_test2, y_pred)\n",
    "rmse = np.sqrt(mse2)\n",
    "r2 = r2_score(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = mean_squared_error(y_test3, y_pred)\n",
    "rmse = np.sqrt(mse3)\n",
    "r2 = r2_score(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "print(\"Average Mean Squared Error of 3 trials:\", (mse1+mse2+mse3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619047619047619\n",
      "Confusion Matrix:\n",
      " [[40  0  0]\n",
      " [ 0 30  3]\n",
      " [ 0  1 31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.97      0.91      0.94        33\n",
      "           2       0.91      0.97      0.94        32\n",
      "\n",
      "    accuracy                           0.96       105\n",
      "   macro avg       0.96      0.96      0.96       105\n",
      "weighted avg       0.96      0.96      0.96       105\n",
      "\n",
      "Accuracy: 0.9714285714285714\n",
      "Confusion Matrix:\n",
      " [[33  0  0]\n",
      " [ 0 34  2]\n",
      " [ 0  1 35]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      0.94      0.96        36\n",
      "           2       0.95      0.97      0.96        36\n",
      "\n",
      "    accuracy                           0.97       105\n",
      "   macro avg       0.97      0.97      0.97       105\n",
      "weighted avg       0.97      0.97      0.97       105\n",
      "\n",
      "Accuracy: 0.9523809523809523\n",
      "Confusion Matrix:\n",
      " [[36  0  0]\n",
      " [ 0 34  3]\n",
      " [ 0  2 30]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.94      0.92      0.93        37\n",
      "           2       0.91      0.94      0.92        32\n",
      "\n",
      "    accuracy                           0.95       105\n",
      "   macro avg       0.95      0.95      0.95       105\n",
      "weighted avg       0.95      0.95      0.95       105\n",
      "\n",
      "average mean of accuracy three trials: 0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep, )\n",
    "\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619047619047619\n",
      "Confusion Matrix:\n",
      " [[40  0  0]\n",
      " [ 0 30  3]\n",
      " [ 0  1 31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.97      0.91      0.94        33\n",
      "           2       0.91      0.97      0.94        32\n",
      "\n",
      "    accuracy                           0.96       105\n",
      "   macro avg       0.96      0.96      0.96       105\n",
      "weighted avg       0.96      0.96      0.96       105\n",
      "\n",
      "Accuracy: 0.9619047619047619\n",
      "Confusion Matrix:\n",
      " [[33  0  0]\n",
      " [ 0 33  3]\n",
      " [ 0  1 35]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      0.92      0.94        36\n",
      "           2       0.92      0.97      0.95        36\n",
      "\n",
      "    accuracy                           0.96       105\n",
      "   macro avg       0.96      0.96      0.96       105\n",
      "weighted avg       0.96      0.96      0.96       105\n",
      "\n",
      "Accuracy: 0.9523809523809523\n",
      "Confusion Matrix:\n",
      " [[36  0  0]\n",
      " [ 0 31  6]\n",
      " [ 0  2 30]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.94      0.84      0.89        37\n",
      "           2       0.83      0.94      0.88        32\n",
      "\n",
      "    accuracy                           0.92       105\n",
      "   macro avg       0.92      0.93      0.92       105\n",
      "weighted avg       0.93      0.92      0.92       105\n",
      "\n",
      "average mean of accuracy three trials: 0.9460317460317459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9428571428571428\n",
      "Confusion Matrix:\n",
      " [[40  0  0]\n",
      " [ 0 30  3]\n",
      " [ 0  3 29]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.91      0.91      0.91        33\n",
      "           2       0.91      0.91      0.91        32\n",
      "\n",
      "    accuracy                           0.94       105\n",
      "   macro avg       0.94      0.94      0.94       105\n",
      "weighted avg       0.94      0.94      0.94       105\n",
      "\n",
      "Accuracy: 0.9523809523809523\n",
      "Confusion Matrix:\n",
      " [[33  0  0]\n",
      " [ 0 33  3]\n",
      " [ 0  2 34]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.94      0.92      0.93        36\n",
      "           2       0.92      0.94      0.93        36\n",
      "\n",
      "    accuracy                           0.95       105\n",
      "   macro avg       0.95      0.95      0.95       105\n",
      "weighted avg       0.95      0.95      0.95       105\n",
      "\n",
      "Accuracy: 0.9523809523809523\n",
      "Confusion Matrix:\n",
      " [[36  0  0]\n",
      " [ 0 34  3]\n",
      " [ 0  4 28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.89      0.92      0.91        37\n",
      "           2       0.90      0.88      0.89        32\n",
      "\n",
      "    accuracy                           0.93       105\n",
      "   macro avg       0.93      0.93      0.93       105\n",
      "weighted avg       0.93      0.93      0.93       105\n",
      "\n",
      "average mean of accuracy three trials: 0.9428571428571427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9619047619047619\n",
      "Confusion Matrix:\n",
      " [[40  0  0]\n",
      " [ 0 31  2]\n",
      " [ 0  2 30]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.94      0.94      0.94        33\n",
      "           2       0.94      0.94      0.94        32\n",
      "\n",
      "    accuracy                           0.96       105\n",
      "   macro avg       0.96      0.96      0.96       105\n",
      "weighted avg       0.96      0.96      0.96       105\n",
      "\n",
      "Accuracy: 0.9523809523809523\n",
      "Confusion Matrix:\n",
      " [[33  0  0]\n",
      " [ 0 33  3]\n",
      " [ 0  2 34]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.94      0.92      0.93        36\n",
      "           2       0.92      0.94      0.93        36\n",
      "\n",
      "    accuracy                           0.95       105\n",
      "   macro avg       0.95      0.95      0.95       105\n",
      "weighted avg       0.95      0.95      0.95       105\n",
      "\n",
      "Accuracy: 0.9428571428571428\n",
      "Confusion Matrix:\n",
      " [[36  0  0]\n",
      " [ 0 32  5]\n",
      " [ 0  1 31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.97      0.86      0.91        37\n",
      "           2       0.86      0.97      0.91        32\n",
      "\n",
      "    accuracy                           0.94       105\n",
      "   macro avg       0.94      0.94      0.94       105\n",
      "weighted avg       0.95      0.94      0.94       105\n",
      "\n",
      "average mean of accuracy three trials: 0.9523809523809522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KNN model with best neighbor found\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train1, y_train1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1 = accuracy_score(y_test1, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test1, y_pred)\n",
    "classification_rep = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2 = accuracy_score(y_test2, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test2, y_pred)\n",
    "classification_rep = classification_report(y_test2, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy2)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy3 = accuracy_score(y_test3, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test3, y_pred)\n",
    "classification_rep = classification_report(y_test3, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy3)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "print(\"average mean of accuracy three trials:\", (accuracy1+accuracy2+accuracy3)/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
